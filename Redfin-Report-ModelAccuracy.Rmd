## Accuracy Evaluation

### Accuracy Methodology

For this evaluation example, we will use one of the neighborhoods: Lents. 
The neighborhood will be selected and each of the models will be trained against the Lents training set and tested for accuracy against the Lents test set.

Mean Absolute Error (MAE) and Root mean squared error (RMSE) are popular to use with forecasting on a single data set, in this case: Lents. MAE and RMSE are scale-dependent, so would be appropriate to compare the models within a single neighborhood. However if we want to compare across neighborhoods, then we would use a metric based on percentage which is more appropriate for the different median house price scales of different neighborhoods. 

Auto Correlation Function (ACF) is a way give correlations between the series at lagged values of the series (seasonality). None of the correlations look significant, which is what we want. 

```{r model-accuracy}

  #select a neighborhood into a dataframe
  oneNeighborhood.df <- neighborhoods.df[Oregon.df$Region == neighborhood,]
  
  #filter out the observations for months that aren't January (1), April (4), July (7), October (10). 
  #Note that had to invert, so selected ones that did not have these patterns matched.
  oneNeighborhood.df<-oneNeighborhood.df[grep("2/|3/|5/|6/|8/|9/|11/|12/", oneNeighborhood.df$Period.Begin, invert = TRUE),]
  # somehow they are out of order, now to sort the period.begin
  oneNeighborhood.df <- oneNeighborhood.df[order(as.Date(oneNeighborhood.df$Period.Begin, format="%m/%d/%Y")),]

  # select the median price column for the timeseries
  oneNeighborhood.df <- oneNeighborhood.df[c(4)]
  
  # create the timeseries
  oneNeighborhood.timeseries <- ts(oneNeighborhood.df, frequency=4, start=c(2012,1))
  
  # divide the timeseries that was created into two parts
  trainingData = window(oneNeighborhood.timeseries, start=c(2012,1), end=c(2015,4)) ### Training set (4 years, quarterly data)
  testData = window(oneNeighborhood.timeseries, start=c(2016,1), end=c(2017,4), frequency=4)  ### Test set    (2 years, quarterly data)
```


For this project we looked at the following types of models: naive, seasonal naive, drift and Holt Winters. 
We run each of these models against the training set for the neighborhood.
  
```{r}
  
  #Run Models on training data
  #Seansonal Naive
  naive.forecast <- naive(trainingData, h = 8)
  snaive.forecast <- snaive(trainingData, h = 8 )
  drift.forecast <- rwf(trainingData, h = 8, drift = T)
  hwinters.forecast <- forecast(HoltWinters(trainingData), h = 8)
```
Next we calculate the accuracy for each of the forecasts of the above models against the test data which was held back from the training set.
```{r}
  #Calculate accuracy
  naive.accuracy <- accuracy(naive.forecast, testData)
  snaive.accuracy <- accuracy(snaive.forecast, testData)
  drift.accuracy <- accuracy(drift.forecast, testData)
  hwinters.accuracy <- accuracy(hwinters.forecast, testData)

```
An easy way to visually determine which of the models matches the test data the closest, is to compare the model forecast against the test data in a graph.
```{r}
  # Forecast plot for comparing models ---------
  plotTitle <- paste(neighborhood, ": Training Model Comparison")
  # plots both the train data, test data and both predictions
  plot(oneNeighborhood.timeseries, main=plotTitle, ylab = "Median House Price", xlab = "Year") 
  lines(naive.forecast$mean,col="brown", lty="dashed")
  lines(snaive.forecast$mean,col="blue", lty="dashed")
  lines(drift.forecast$mean,col="green", lty="dashed")
  lines(hwinters.forecast$mean, col="orange", lty="dashed")
  lines(testData, col="red") # color the test data to tell it from the train data
  legend("topleft",lty=1,bty = "n",col=c("black", "red", "brown","blue", "green", "orange"),c("Train Data", "Test Data","Naive","Seasonal Naive","Drift", "Holt Winters"))
```
We can see in this graph, that the dotted orange line, for Holt Winters most closely follows the red solid line representing the test data. The Naive line is useless as it is simply the last value in the Training data. The green line for Drift seems to follow our test data as a trend line rather well. The Seasonal Naive line is somewhat better than pure naive, as it generally follows the seasonality of the test data, if not the scale.

Now let's look at the actual accuracy for each of the models for this neighborhood.

### Seasonal Naive

We can see that seasonal naive has a RMSE of 30584 for the training set and 58482 for the test set. 

The ACF is 0.3094 for training and 0.4584 for the test set. 
```{r}
  # Seasonal Naive
  print(paste("Seasonal Naive Accuracy for ", neighborhood))
  print(snaive.accuracy)  
  plotTitle <- paste(neighborhood, ": Seasonal Naive Model Residuals")
  options(scipen=999)
  tsdisplay(residuals(naive.forecast), main = paste(plotTitle, " Residuals"))
```

###  Naive
Naive has a RMSE of 11738.18 for the training set and 49342.8 for the test set. 

The ACF is -0.2522954 for training and 0.5108548 for the test set. 

```{r} 
#  Naive
  print(paste("Naive Accuracy for ", neighborhood))
  print(naive.accuracy)
  plotTitle <- paste(neighborhood, ": Naive Model Residuals")
  options(scipen=999)
  tsdisplay(residuals(naive.forecast), main = paste(plotTitle, " Residuals"))
```

### Holt Winters 
Holt Winters has a RMSE of 10710.34 for the training set and 9232.58 for the test set. 

The ACF is 0.2502565 for training and -0.357791 for the test set. 

This is much better than seasonal Naive or Naive.
```{r}
  # Holt Winters
  print(paste("Holt Winters Accuracy for ", neighborhood))
  print(hwinters.accuracy)
  plotTitle <- paste(neighborhood, ": Holt Winters Model Residuals")
  options(scipen=999)
  tsdisplay(residuals(hwinters.forecast), main = paste(plotTitle, " Residuals"))
```
### Drift 
Drift  has a RMSE of 9710.479 for the training set and  18819.398 for the test set. 

The ACF is  -0.25229542 for training and  -0.0709451 for the test set. 

This is much better than seasonal Naive or Naive.
```{r}
    # Drift
  print(paste("Drift Accuracy for ", neighborhood))
  print(drift.accuracy)
  plotTitle <- paste(neighborhood, ": Drift Model Residuals")
  options(scipen=999)
  tsdisplay(residuals(drift.forecast), main = paste(plotTitle, " Residuals"))

```

Lents Neighborhood accuracy metrics: 

Model        | Training RMSE | Test set RMSE | Training ACF | Test ACF
------------ | ------------- | ------------- | ------------ | --------
Naive        | 11738.18	     | 49342.8       | -0.2522954   | 0.5108548
Seasonal Naive	| 30584      | 58482         | 0.3094       | 0.4584
Drift        | 	9710.479     | 18819.398     | -0.25229542  | -0.0709451
Holt Winters | 	10710.34     | 9232.58       |  0.2502565   | -0.357791


For accurate forecasting, we have chosen Holt Winters as the most accurate model, for this neighborhood as well as a random sampling of others in the set of neighborhoods. The test set RMSE was the best for this model. Notice that Holt Winters has a test set RMSE of 9232. This is the least RMSE for any of the models. For Holt Winters the training RMSE was only very slightly higher than Drift, and slightly under Naive. Suprisingly, Seasonal Naive was significantly off, but not as bad overall as general naive modeling.

For this paper, we have chosen to compare Holt Winters, our most accurate model, against Seasonal Naive. Seasonal Naive is the most accurate of the two naive models and similar to Holt Winters accounts for seasonality. Drift is a useful trend line but doesn't give the detail that holt winters provides for seasonal dips and peaks. It may be useful for determining generalized trends in home prices, which for some consumers of this data may be enough (ie. homeowners wanting to know if their property is going up or down in price, but not looking to sell in a certain season).

Recommended future work, which this project did not have time for, would be to calculate the mean of the RMSE values for each model across the entire set of neighborhoods. This would allow us to more accurately examine which model fit the most neighborhoods properly, which our sample could not properly extrapolate. By adding the Training RMSE and Test RMSE to an array through rbind, which was updated on each pass through the for-loop (from the appendix), we could compare the RMSE values across neighborhoods effectively.
